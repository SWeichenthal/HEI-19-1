{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('set_your_working_directory/')\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 13 14:16:07 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA TITAN Xp     Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 23%   33C    P8    10W / 250W |  11668MiB / 12194MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN Xp     Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 23%   33C    P8    11W / 250W |    267MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA TITAN Xp     Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 23%   32C    P8     9W / 250W |    267MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA TITAN Xp     Off  | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 23%   31C    P8     9W / 250W |    267MiB / 12196MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1507      G   /usr/lib/xorg/Xorg                 27MiB |\n",
      "|    0   N/A  N/A      1745      G   /usr/bin/gnome-shell               56MiB |\n",
      "|    0   N/A  N/A      2157      C   ...a3/envs/tf-gpu/bin/python    11579MiB |\n",
      "|    1   N/A  N/A      1507      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A      2157      C   ...a3/envs/tf-gpu/bin/python      259MiB |\n",
      "|    2   N/A  N/A      1507      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A      2157      C   ...a3/envs/tf-gpu/bin/python      259MiB |\n",
      "|    3   N/A  N/A      1507      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A      2157      C   ...a3/envs/tf-gpu/bin/python      259MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check the GPUs, make sure they are not being used\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Root Mean Squared Error Loss Function\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.backend.sqrt(K.backend.mean(K.backend.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "### nested loop, does all 3 pollutants at various cut offs for a particular set of cut offs  #####\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the city, exposure and the images for training. Note 'size' is ufp mean size\n",
    "city = input(\"Which city: to or mtl? \")\n",
    "m_center = 'median'\n",
    "p_filter_type = 'days'\n",
    "image_type = \"satellite\"\n",
    "sat_zoom = input(\"Which satellite zoom? 18, 19, or both? \")\n",
    "zoom_angle_1 = 'images_18'\n",
    "zoom_angle_2 = 'images_19'\n",
    "image_file_name = 'sat_file'\n",
    "vflip = True\n",
    "\n",
    "# sometimes it's better to train on quintiles or deciles. our monitoring campaign was extensive and well balances, so we found that training on continous worked well and there was no need to try quintiles or deciles\n",
    "p_category = 'continous'\n",
    "\n",
    "# set the max number of epochs to train. Good to start with at least 20. Then if it's working well, go to 100 and train overnight\n",
    "num_epochs = input(\"Max number of epochs? \")\n",
    "\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "if num_epochs > 50:\n",
    "    print('!!! Are you sure you want to train up to ' + str(num_epochs) + 'epochs?')\n",
    "\n",
    "# loop will train models for each pollutant. 'ufp', 'size' and 'bc' are the names of separate columns in the metadata that indicate the ufp number concentration, mean ufp size, and bc mass concentrations\n",
    "pollutant = ['ufp', 'size', 'bc']\n",
    "\n",
    "# loop will also train models using various cut offs for minimum number of days of monitoring\n",
    "p_all_filter_levels = [0, 2, 4, 6, 8, 10] # you can use fewer cut offs in order to speed up training\n",
    "\n",
    "num_epochs = int(num_epochs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pollutant:\n",
    "\n",
    "    tv_class_mode = 'raw'\n",
    "    test_class_mode = None \n",
    "    if i == 'size':\n",
    "        exposure = i + '_' + m_center\n",
    "    else:\n",
    "        exposure = 'log_' + i + '_' + m_center # ufp and bc concentrations have been log-transformed in the metadata. the ufp and bc concentration columns in the metadata are called \"log_ufp_median\" and \"log_ufp_median\" respectively\n",
    "    \n",
    "    if i == \"bc\":\n",
    "        p_filter = p_filter_type + '_' + i\n",
    "    if i == \"ufp\":\n",
    "        p_filter = p_filter_type + '_' + i\n",
    "    if i == \"size\":\n",
    "        p_filter = p_filter_type + '_ufp'\n",
    "    \n",
    "    if city == \"to\":\n",
    "        dat = pd.read_csv(filepath_or_buffer = \"data_files/images/to_images/model_development_images/t100_new_image_metadata.csv\")\n",
    "\n",
    "    if city == \"mtl\":\n",
    "        dat = pd.read_csv(filepath_or_buffer = \"data_files/images/mtl_images/model_development_images/m100_new_image_metadata.csv\")\n",
    "\n",
    "    # remove excess columns from metadata\n",
    "    if i == 'size':\n",
    "        dat = dat[[image_file_name, 'set_ghp6', p_filter, exposure, 'temp', 'hum', 'ws']]\n",
    "    else:\n",
    "        dat = dat[[image_file_name, 'set_ghp6', p_filter, exposure, i + '_' + m_center, 'temp', 'hum', 'ws']]\n",
    "\n",
    "    # subset to observations where exposure is nonmissing\n",
    "    dat = dat[dat[exposure].notnull()].reset_index(drop=True)\n",
    "\n",
    "    dat[[exposure]] = dat[[exposure]].astype(\"float32\")\n",
    "    \n",
    "    # this is to reset image_type to satellite if it was set to satellite18 or satellite19 for the files\n",
    "    image_type = 'satellite'\n",
    "\n",
    "    if sat_zoom == 'both':\n",
    "        s1, s2 = dat.copy(), dat.copy()\n",
    "\n",
    "        s1[image_file_name] = city+'_' +zoom_angle_1+'/' + s1[image_file_name]\n",
    "        s2[image_file_name] = city+'_' +zoom_angle_2+'/' + s2[image_file_name]\n",
    "\n",
    "        dat = pd.concat([s1, s2])\n",
    "    else:\n",
    "        if sat_zoom == '18':\n",
    "            dat[image_file_name] = city+'_' +zoom_angle_1+'/' + dat[image_file_name]\n",
    "        else:\n",
    "            if sat_zoom == '19':\n",
    "                dat[image_file_name] = city+'_' +zoom_angle_2+'/' + dat[image_file_name]\n",
    "        \n",
    "    for j in p_all_filter_levels:\n",
    "        \n",
    "        # this is to reset image_type to satellite if it was set to satellite18 or satellite19 for the files\n",
    "        image_type = 'satellite'\n",
    "        \n",
    "        # remove observations below the cut off\n",
    "        dat = dat[dat[p_filter] >= j]\n",
    "\n",
    "\n",
    "        generator = K.preprocessing.image.ImageDataGenerator(preprocessing_function=K.applications.xception.preprocess_input,\n",
    "                                                             horizontal_flip=True, \n",
    "                                                             vertical_flip = vflip)\n",
    "\n",
    "\n",
    "        train_generator = generator.flow_from_dataframe(dataframe=dat.loc[dat['set_ghp6']=='train', [exposure, image_file_name]].reset_index(drop=True),\n",
    "                                                        directory='data_files/images/'+city+'_images/model_development_images/'+image_type+'_view_100m/',\n",
    "                                                        x_col= image_file_name,\n",
    "                                                        y_col=exposure,\n",
    "                                                        #has_ext=True, #depreciated, not needed because we already have the extension on our filenames\n",
    "                                                        class_mode=tv_class_mode,\n",
    "                                                        target_size=(256, 256),\n",
    "                                                        color_mode='rgb',\n",
    "                                                        batch_size=32*4,\n",
    "                                                        shuffle=True)\n",
    "\n",
    "        validate_generator = generator.flow_from_dataframe(dataframe=dat.loc[dat['set_ghp6']=='validate', [exposure, image_file_name]].reset_index(drop=True),\n",
    "                                                             directory='data_files/images/'+city+'_images/model_development_images/'+image_type+'_view_100m/',\n",
    "                                                             x_col= image_file_name,\n",
    "                                                             y_col=exposure,\n",
    "                                                             #has_ext=True, #depreciated, not needed because we already have the extension on our filenames\n",
    "                                                             class_mode=tv_class_mode,\n",
    "                                                             target_size=(256, 256),\n",
    "                                                             color_mode='rgb',\n",
    "                                                             batch_size=32*4,\n",
    "                                                             shuffle=False)\n",
    "\n",
    "        test_generator = generator.flow_from_dataframe(dataframe=dat.loc[dat['set_ghp6']=='test', [exposure, image_file_name]].reset_index(drop=True),\n",
    "                                                       directory='data_files/images/'+city+'_images/model_development_images/'+image_type+'_view_100m/',\n",
    "                                                       x_col= image_file_name,\n",
    "                                                       #has_ext=True, #depreciated, not needed because we already have the extension on our filenames\n",
    "                                                       class_mode= test_class_mode,\n",
    "                                                       target_size=(256, 256),\n",
    "                                                       color_mode='rgb',\n",
    "                                                       batch_size=32*4,\n",
    "                                                       shuffle=False)\n",
    "\n",
    "        if sat_zoom == 'both':\n",
    "            image_type = image_type\n",
    "        else:\n",
    "            image_type = image_type+sat_zoom  # in order to save files as separate from dual satellite           \n",
    "        \n",
    "        # set callbacks\n",
    "        early_stopping = K.callbacks.EarlyStopping(monitor='val_rmse', patience=15)\n",
    "        reduce_lr_on_plateau = K.callbacks.ReduceLROnPlateau(monitor='val_rmse', factor=0.1, patience=5, mode='min', verbose=1)\n",
    "        csv_logger = K.callbacks.CSVLogger('model_development/model_logs/'+city+'_'+image_type+'/'+city+', '+exposure+', '+p_filter+' '+str(j)+'o, '+image_type+', 10e-4.csv')\n",
    "        model_checkpoint = K.callbacks.ModelCheckpoint('model_development/models/'+city+'_'+image_type+'/'+city+', '+exposure+', '+p_filter+' '+str(j)+'o, '+image_type+', 10e-4.hdf5', monitor='val_rmse', mode='min', save_weights_only=False, save_best_only=True)\n",
    "\n",
    "        # define continous model\n",
    "        def get_compiled_model():\n",
    "            # define model\n",
    "            model_input = K.layers.Input(shape=(256, 256, 3), dtype='float32', name='input')\n",
    "            conv_base = K.applications.Xception(include_top=False, weights='imagenet', input_tensor=model_input)\n",
    "            model_output = K.layers.GlobalAveragePooling2D()(conv_base.output)\n",
    "            model_output = K.layers.Dense(units=1, activation='linear')(model_output)\n",
    "            model = K.models.Model(inputs=model_input, outputs=model_output)\n",
    "            model.compile(\n",
    "                optimizer=K.optimizers.Nadam(lr=0.001),\n",
    "                loss = rmse,   # or MeanSquaredError(), or K.losses.MeanAbsoluteError(reduction=\"auto\", name=\"mean_absolute_error\")\n",
    "                metrics = [rmse,'mae']\n",
    "            )\n",
    "            return model\n",
    "\n",
    "        # Create a MirroredStrategy.\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "        # Open a strategy scope.\n",
    "        with strategy.scope():\n",
    "            model = get_compiled_model()\n",
    "\n",
    "        # Train the model on all available devices.\n",
    "        for layer in model.layers: layer.trainable = True\n",
    "\n",
    "        model.fit(train_generator, \n",
    "                  validation_data=validate_generator,\n",
    "                  epochs=num_epochs, \n",
    "                  steps_per_epoch=int(np.ceil(train_generator.samples/train_generator.batch_size)),\n",
    "                  validation_steps=int(np.ceil(validate_generator.samples/validate_generator.batch_size)),\n",
    "                  callbacks=[early_stopping, reduce_lr_on_plateau, csv_logger, model_checkpoint])\n",
    "\n",
    "        # load last model continuous and generate predictions in the test set\n",
    "        model = K.models.load_model('model_development/models/'+city+'_'+image_type+'/'+city+', '+exposure+', '+p_filter+' '+str(j)+'o, '+image_type+', 10e-4.hdf5', custom_objects={'rmse': rmse})\n",
    "\n",
    "        results = dat.loc[dat['set_ghp6']=='test', [image_file_name, 'set_ghp6', 'temp', 'hum', 'ws', i + '_' + m_center, exposure]].copy().rename(columns={'file': 'File', 'set_ghp6': 'Set', 'temp': 'Temp', 'hum': 'Hum', 'ws': 'Wind_Speed', i + '_' + m_center: i + '_' + m_center, exposure: exposure}).reset_index(drop=True)\n",
    "\n",
    "        results[exposure+'_pred'] = model.predict(x=test_generator, \n",
    "                                                               steps=int(np.ceil(test_generator.samples/test_generator.batch_size)))\n",
    "        results.to_csv(path_or_buf='model_development/model_predictions/'+city+'_'+image_type+'/'+city+', '+exposure+', '+p_filter+' '+str(j)+'o, '+image_type+', 10e-4.csv', index=False)\n",
    "        results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "######## Generate predictions ###############\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the city, exposure and the images for training. Note 'size' is ufp mean size\n",
    "city = input(\"Which city: to or mtl? \")\n",
    "m_center = 'median'\n",
    "p_filter_type = 'days'\n",
    "image_type = 'satellite'\n",
    "\n",
    "sat_zoom = input(\"Which satellite zoom? 18, 19, or both? \")\n",
    "\n",
    "# can generate predictions on all the model development images (i.e. for the monitoring sites) or for the prediction surface (i.e. the fishnet, all the 100 m x 100 m cells in the study area)\n",
    "dev_or_pred_images = input(\"Use which set of images? prediction or development? \")        \n",
    "\n",
    "pollutant = ['ufp', 'size', 'bc']\n",
    "\n",
    "p_all_filter_levels = [0, 2, 4, 6, 8, 10] # here is where I can change which cutoffs are used\n",
    "\n",
    "zoom_angle_1 = 'images_18'\n",
    "zoom_angle_2 = 'images_19'\n",
    "image_file_name = 'sat_file'\n",
    "vflip = True\n",
    "\n",
    "if dev_or_pred_images == 'prediction':\n",
    "    pred_image_gis_file = 'fishnet'\n",
    "    end_image_folder_name = 'fishnet' \n",
    "else:\n",
    "    if dev_or_pred_images == 'development':\n",
    "        pred_image_gis_file = '100m_new_dev' \n",
    "        end_image_folder_name = '100m'      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pollutant:\n",
    "    tv_class_mode = 'raw'\n",
    "    test_class_mode = None \n",
    "    \n",
    "    if i == 'size':\n",
    "        exposure = i + '_' + m_center\n",
    "    else:\n",
    "        exposure = 'log_' + i + '_' + m_center\n",
    "    \n",
    "    # I still need this because it's the names of the models\n",
    "    if i == \"bc\":\n",
    "        p_filter = p_filter_type + '_' + i\n",
    "    if i == \"ufp\":\n",
    "        p_filter = p_filter_type + '_' + i\n",
    "    if i == \"size\":\n",
    "        p_filter = p_filter_type + '_ufp'\n",
    "    \n",
    "    if city == \"to\":\n",
    "        if dev_or_pred_images == 'prediction':\n",
    "            dat = pd.read_csv(filepath_or_buffer = \"data_files/images/to_images/model_prediction_images/t_fishnet_image_metadata.csv\")\n",
    "        else:\n",
    "            dat = pd.read_csv(filepath_or_buffer = \"data_files/images/to_images/model_development_images/t100_new_image_metadata.csv\")                \n",
    "            dat.rename(columns = {'point_lon':'lon', 'point_lat':'lat'}, inplace = True)\n",
    "\n",
    "\n",
    "    if city == \"mtl\":\n",
    "        if dev_or_pred_images == 'prediction':\n",
    "            dat = pd.read_csv(filepath_or_buffer = \"data_files/images/mtl_images/model_prediction_images/m_fishnet_image_metadata.csv\")\n",
    "        else:\n",
    "            dat = pd.read_csv(filepath_or_buffer = \"data_files/images/mtl_images/model_development_images/m100_new_image_metadata.csv\")   \n",
    "            dat.rename(columns = {'point_lon':'lon', 'point_lat':'lat'}, inplace = True)                \n",
    "\n",
    "    # remove excess columns\n",
    "    dat = dat[[image_file_name, 'site_id', 'lon', 'lat']]\n",
    "   \n",
    "\n",
    "    if sat_zoom == 'both':\n",
    "        s1, s2 = dat.copy(), dat.copy()\n",
    "\n",
    "        s1[image_file_name] = 'model_'+dev_or_pred_images+'_images/'+image_type+'_view_'+end_image_folder_name+'/'+city+'_' +zoom_angle_1+'/' + s1[image_file_name]\n",
    "        s2[image_file_name] = 'model_'+dev_or_pred_images+'_images/'+image_type+'_view_'+end_image_folder_name+'/'+city+'_' +zoom_angle_2+'/' + s2[image_file_name]\n",
    "\n",
    "        dat = pd.concat([s1, s2])\n",
    "    else:\n",
    "        if sat_zoom == '18':\n",
    "            dat[image_file_name] = 'model_'+dev_or_pred_images+'_images/'+image_type+'_view_'+end_image_folder_name+'/'+city+'_' +zoom_angle_1+'/' + dat[image_file_name]\n",
    "        else:\n",
    "            if sat_zoom == '19':\n",
    "                dat[image_file_name] = 'model_'+dev_or_pred_images+'_images/'+image_type+'_view_'+end_image_folder_name+'/'+city+'_' +zoom_angle_2+'/' + dat[image_file_name]\n",
    "\n",
    "    generator = K.preprocessing.image.ImageDataGenerator(preprocessing_function=K.applications.xception.preprocess_input,\n",
    "                                                         horizontal_flip=False, \n",
    "                                                         vertical_flip = False) \n",
    "\n",
    "    test_generator = generator.flow_from_dataframe(dataframe=dat,\n",
    "                                                   directory='data_files/images/'+city+'_images/',\n",
    "                                                   x_col= image_file_name,\n",
    "                                                   #has_ext=True, #depreciated, not needed because we already have the extension on our filenames\n",
    "                                                   class_mode= test_class_mode,\n",
    "                                                   target_size=(256, 256),\n",
    "                                                   color_mode='rgb',\n",
    "                                                   batch_size=32*4,\n",
    "                                                   shuffle=False)\n",
    "\n",
    "        # load models\n",
    "    model6 = K.models.load_model('model_development/models/'+city+'_'+image_type+'/'+city+', '+exposure+', '+p_filter+' 6o, '+image_type+', 10e-4.hdf5', custom_objects={'rmse': rmse})\n",
    "    model8 = K.models.load_model('model_development/models/'+city+'_'+image_type+'/'+city+', '+exposure+', '+p_filter+' 8o, '+image_type+', 10e-4.hdf5', custom_objects={'rmse': rmse})\n",
    "    \n",
    "    model0 = K.models.load_model('model_development/models/'+city+'_'+image_type+'/'+city+', '+exposure+', '+p_filter+' 0o, '+image_type+', 10e-4.hdf5', custom_objects={'rmse': rmse})\n",
    "    model2 = K.models.load_model('model_development/models/'+city+'_'+image_type+'/'+city+', '+exposure+', '+p_filter+' 2o, '+image_type+', 10e-4.hdf5', custom_objects={'rmse': rmse})\n",
    "    model4 = K.models.load_model('model_development/models/'+city+'_'+image_type+'/'+city+', '+exposure+', '+p_filter+' 4o, '+image_type+', 10e-4.hdf5', custom_objects={'rmse': rmse})\n",
    "    model10 = K.models.load_model('model_development/models/'+city+'_'+image_type+'/'+city+', '+exposure+', '+p_filter+' 10o, '+image_type+', 10e-4.hdf5', custom_objects={'rmse': rmse})\n",
    "    \n",
    "\n",
    "    results = dat\n",
    "\n",
    "    results[exposure+'_pred_0o'] = model6.predict(x=test_generator, steps=int(np.ceil(test_generator.samples/test_generator.batch_size)))\n",
    "    results[exposure+'_pred_2o'] = model6.predict(x=test_generator, steps=int(np.ceil(test_generator.samples/test_generator.batch_size)))\n",
    "    results[exposure+'_pred_4o'] = model6.predict(x=test_generator, steps=int(np.ceil(test_generator.samples/test_generator.batch_size)))\n",
    "    results[exposure+'_pred_6o'] = model6.predict(x=test_generator, steps=int(np.ceil(test_generator.samples/test_generator.batch_size)))\n",
    "    results[exposure+'_pred_8o'] = model8.predict(x=test_generator, steps=int(np.ceil(test_generator.samples/test_generator.batch_size)))\n",
    "    results[exposure+'_pred_10o'] = model6.predict(x=test_generator, steps=int(np.ceil(test_generator.samples/test_generator.batch_size)))\n",
    "    \n",
    "    results.to_csv(path_or_buf='model_development/model_predictions/'+city+'_'+pred_image_gis_file+'_'+image_type+'/'+city+', '+exposure+', '+p_filter+' '+image_type+', 10e-4.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
